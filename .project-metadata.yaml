name: Llama2-7B Standalone Model Deployment (Gov Edition)
description: This AMP deploys Llama2-7b model as a CML model endpoint, callable via an API. Model is hosted within CML and requires GPU node with 16GB memory and 4 cores minimum.
author: Cloudera
date: "2025-1-20"
specification_version: 1.0
prototype_version: 1.0

environment_variables:
  HF_ACCESS_TOKEN:
    default: ""
    description: "Access Token to use Llama2 7B model"
    required: true
  PATH:
    default: "${PATH}:$HOME/cuda/bin:$HOME/cuda/nvvm"
    description: "Leave this value as is"
    required: true
  LD_LIBRARY_PATH:
    default: "${LD_LIBRARY_PATH}:$HOME/cuda/lib64:$HOME/cuda/nvvm/lib64:$HOME/cuda/nvvm"
    description: "Leave this value as is"
    required: true
  CUDA_HOME:
    default: "$HOME/cuda"
    description: "Leave this value as is"
    required: true

runtimes: 
  - editor: PBJ Workbench
    kernel: Python 3.9
    edition: GovCloud
  
tasks:
  - type: create_model
    name: Llama2-7b
    entity_label: llama2-7b
    description: Llama2-7b model hosted in CML. 
    short_summary: Llama2-7b
    default_resources:
      cpu: 4
      memory: 16
      gpu: 1
    default_replication_policy:
      type: fixed
      num_replicas: 1
  
  - type: build_model
    name: Build Llama2 7B model
    entity_label: llama2-7b
    comment: First build by the AMP
    examples:
      - request:
          prompt: What is Cloudera?
          temperature: 0
          max_new_tokens: 50
          repetition_penalty: 0.5

    target_file_path: Launch_model.py
    target_function_name: api_wrapper

  - type: deploy_model
    entity_label: llama2-7b